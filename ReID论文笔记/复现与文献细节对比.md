# 复现与文献细节对比

### 1. 文献中的模型和方法概述

#### 1.1 模型架构
- **Backbone**：基于 ResNet-50 的双流网络（two-stream ResNet-50）。
- **DEE 模块**（Diverse Embedding Expansion Module）：
  - 在 embedding 空间生成多样化嵌入。
  - 默认插入在 ResNet-50 的 **stage-3** 之后（见 Table 5）。
  - 使用多分支卷积结构（默认 3 个分支，见 Table 6），包括扩张卷积和 1×1 卷积。
- **MFA 块**（Multistage Feature Aggregation Block）：
  - 聚合不同阶段的特征，挖掘通道和空间多样化表示。

#### 1.2 损失函数
- **CPM 损失**（Center-Guided Pair Mining Loss）：
  - 驱动 DEE 生成多样化嵌入，减少模态差异（见 Eq. (5)）。
- **正交损失**（Orthogonal Loss）：
  - 确保不同分支嵌入正交（见 Eq. (6)）。
- **交叉熵损失和三元组损失**：
  - 联合优化网络。
- **总损失**：
  ```
  L_total = L_ce + L_tri + λ1 * L_cpm + λ2 * L_ort
  ```
  - `λ1 = 0.8`，`λ2 = 0.1`（见 Section 5.4）。

#### 1.3 数据集
- **SYSU-MM01**：491 个身份，4 个 VIS 相机，2 个 IR 相机。
- **RegDB**：412 个身份，每人 10 张 VIS 和 10 张 IR 图像。
- **LLCM**：1064 个身份，46767 张图像，9 个 VIS/IR 相机，低光照环境。

#### 1.4 实现细节
- **输入图像大小**：`384×144`（文献中提到 `3×384×144`，但应为 `288×144` 或 `384×144`，可能有笔误）。
- **数据增强**：随机水平翻转和随机擦除（`erasing_p` 未明确值，常见为 0.5）。
- **优化器**：SGD，动量 0.9。
- **学习率**：
  - 初始 `0.01`，10 epoch 后 warm-up 至 `0.1`。
  - 在 20、60、120 epoch 衰减至 `0.01`、`0.001`、`0.0001`，共 150 epoch。
- **批次大小**：
  - 每个 mini-batch 包含 6 个身份，每个身份 4 张 VIS 和 4 张 IR 图像（`batch_size=6`，`num_pos=4`）。
- **DEE 插入**：默认在 stage-3 后。

---

### 2. 你的复现情况

#### 2.1 实验流程
- **LLCM 数据集**：
  - **训练**：`python train.py --dataset llcm --gpu 6`
  - **测试**：`python test.py --mode all --tvsearch True --resume 'llcm_deen_p4_n6_lr_0.1_seed_0_best.t' --gpu 1 --dataset llcm`
  - **生成模型**：`llcm_deen_p4_n6_lr_0.1_seed_0_best.t`
- **SYSU-MM01 数据集**：
  - **训练**：`python train.py --dataset sysu --gpu 6`
  - **参数**：
    ```
    Args:Namespace(
      arch='resnet50', batch_size=6, dataset='sysu', erasing_p=0.5, gpu='6',
      img_h=288, img_w=144, lambda_1=0.8, lambda_2=0.01, log_path='log/',
      lr=0.1, margin=0.3, mode='all', model_path='save_model/', num_pos=4,
      optim='sgd', resume='', save_epoch=20, seed=0, test_batch=4, test_only=False,
      trial=2, vis_log_path='log/vis_log/', workers=0
    )
    ```
  - 未使用 LLCM 模型（`resume=''`）。

#### 2.2 代码分析
- **`loss.py`**：
  - **`CPMLoss`**：实现 CPM 损失，与文献 Eq. (3)-(5) 一致。
  - **`OriTripletLoss`**：实现三元组损失，与文献 `L_tri` 一致。
  - **缺失部分**：未见交叉熵损失 (`L_ce`) 和正交损失 (`L_ort`)，可能在 `train.py` 中。
  - **`pdist_torch`**：计算欧氏距离，与 CPM 损失定义匹配。
- **参数匹配**：
  - `batch_size=6`, `num_pos=4`：与文献一致。
  - `lr=0.1`：与 warm-up 后一致，但未见调度。
  - `lambda_1=0.8`：正确。
  - `lambda_2=0.01`：应为 `0.1`，不一致。
  - `img_h=288, img_w=144`：与文献 `384×144` 不符。
  - `erasing_p=0.5`：合理，文献未明确。

---

### 3. 判断你的做法是否正确

#### 3.1 正确的地方
1. **模型架构**：
   - `arch='resnet50'` 与文献的双流 ResNet-50 一致。
   - `loss.py` 中的 `CPMLoss` 和 `OriTripletLoss` 实现了核心损失。
2. **数据集**：
   - 在 LLCM 和 SYSU-MM01 上训练和测试，与文献一致。
3. **批次配置**：
   - `batch_size=6`, `num_pos=4` 完全匹配。
4. **优化器**：
   - `optim='sgd'` 正确。
5. **损失权重**：
   - `lambda_1=0.8` 正确。

#### 3.2 不正确或需调整的地方
1. **图像大小**：
   - **你的设置**：`img_h=288, img_w=144`。
   - **文献要求**：`384×144`（文献可能笔误为 `3×384×144`）。
   - **问题**：`288×144` 与推荐的 `384×144` 不符，可能影响特征提取。建议调整为 `384×144`。
2. **学习率调度**：
   - **你的设置**：`lr=0.1`，未见 warm-up 或衰减。
   - **文献要求**：初始 `0.01`，10 epoch 后 `0.1`，20、60、120 epoch 衰减至 `0.01`、`0.001`、`0.0001`，共 150 epoch。
   - **问题**：若 `train.py` 未实现完整调度，训练过程不符。需检查或添加调度逻辑。
3. **损失函数完整性**：
   - **你的代码**：有 `CPMLoss` 和 `OriTripletLoss`，缺 `L_ce` 和 `L_ort`。
   - **文献要求**：`L_total = L_ce + L_tri + λ1 * L_cpm + λ2 * L_ort`。
   - **问题**：若 `train.py` 未补全损失，模型优化不完整。需确认并添加。
4. **正交损失权重**：
   - **你的设置**：`lambda_2=0.01`。
   - **文献要求**：`λ2=0.1`。
   - **问题**：权重偏低，可能削弱正交约束，建议改为 `0.1`。
5. **SYSU-MM01 从零训练**：
   - **你的做法**：未加载 LLCM 模型，从头训练。
   - **文献做法**：未强制要求迁移，但微调常见。
   - **问题**：从零训练技术上无误，但若目标是高效，可用 LLCM 模型微调。

#### 3.3 SYSU-MM01 训练的具体分析
- **命令**：`python train.py --dataset sysu --gpu 6`。
- **日志**：
  - 数据加载：`test_ir_resized_img.npy` 等，统计（395 个身份，22258 张 VIS，11909 张 IR）与文献一致。
  - 错误：`RuntimeError: self and mat2 must have the same dtype`，已通过修改 `loss.py` 解决。
- **正确性**：除参数偏差外，流程基本符合。

---

### 4. 结论
- **总体评价**：你的复现抓住了 DEEN 的核心（ResNet-50、DEE、CPM、三元组损失），在 LLCM 和 SYSU-MM01 上运行，方向正确。
- **需改进**：
  1. 图像大小：`288×144` → `384×144`。
  2. 学习率：添加 warm-up 和衰减。
  3. 损失：补全交叉熵和正交损失。
  4. `lambda_2`：`0.01` → `0.1`。
  5. 可选：SYSU-MM01 用 LLCM 模型微调。

---

### 5. 改进建议

#### 5.1 调整参数
```bash
python train.py --dataset sysu --gpu 6 --img_h 384 --img_w 144 --lambda_2 0.1
```
- 在 `train.py` 中确认学习率调度和所有损失。

#### 5.2 验证完整性
- **交叉熵损失**：
  - 添加 `nn.CrossEntropyLoss`，确保 `L_ce` 计算。
- **正交损失**：
  - 实现 Eq. (6)：
    ```python
    def orthogonal_loss(f_v):
        loss = 0
        for m in range(len(f_v)-1):
            for n in range(m+1, len(f_v)):
                loss += torch.dot(f_v[m].T, f_v[n])
        return loss
    ```
  - 在 `train.py` 中加入：
    ```python
    L_ort = orthogonal_loss(generated_embeddings)
    L_total = L_ce + L_tri + args.lambda_1 * L_cpm + args.lambda_2 * L_ort
    ```

#### 5.3 对比结果
- 训练后用 `test.py` 评估 SYSU-MM01 的 mAP 和 Rank-1，与 Table 2 对比：
  ```bash
  python test.py --mode all --tvsearch True --resume 'sysu_deen_p4_n6_lr_0.1_seed_0_best.t' --gpu 1 --dataset sysu
  ```

#### 5.4 微调尝试
- 用 LLCM 模型微调 SYSU-MM01：
  ```bash
  python train.py --dataset sysu --gpu 6 --resume 'llcm_deen_p4_n6_lr_0.1_seed_0_best.t' --lr 0.01
  ```

---

### 6. 下一步
- **提供更多代码**：若能分享 `train.py` 的损失计算和学习率调度部分，我可进一步确认。
- **当前状态**：你的复现已接近原文，微调后可完全符合要求。
有什么疑问随时告诉我！